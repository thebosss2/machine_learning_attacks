Yeah, I liked the Neural Cleanse paper. Their method for detecting backdoors by finding the 'easiest' target label to attack is pretty intuitive, and they showed it was quite effective across different models in their experiments. They didn't just stop at detection either; their mitigation techniques also seemed to perform well. It was interesting to see how they reverse engineer the triggers and then use that for the cleanup.

This paper is all about the process of finding and fixing backdoor attacks that might be present in deep neural networks. Their main approach for this involves trying to reverse-engineer the smallest possible 'trigger' for each class and then seeing if any class has a trigger that's suspiciously small compared to the others, which acts as a signal. Building on that, they're studying how you can systematically detect if a model has a backdoor, figure out what the trigger actually looks like, and then how to remove or at least neutralize the backdoor without messing up the model's normal job.

A major one is that they cover the whole lifecycle of dealing with backdoors, not just detecting them, but also identifying what the trigger looks like and then offering several ways to actually mitigate the attack, like neuron pruning or unlearning. Their core detection method, based on reverse engineering minimal triggers and then looking for outliers, is pretty intuitive and seems to be generalizable across different models(and works pretty well). They also backed up their claims with a lot of experiments on various datasets and against different types of backdoor attacks, including some advanced ones, which makes their findings more solid. And it's good they even thought about making detection less computationally heavy for really big models.

First off, their method is mainly shown for image tasks, so how well it applies to other things like text or audio isn't fully clear. Second, the detection part can get pretty slow and resource-heavy, especially for models with a lot of categories or when trying to find those subtle, partial backdoors. Third, their ways of fixing the model sometimes really rely on how accurately they can reverse-engineer the original trigger, and that's not always a perfect match. Lastly, as they even mention, clever attackers can still find ways to challenge their core defense assumptions, for example, by using super complex triggers or targeting many classes.

I believe there are actually tons of ways to continue this cool paper. If I were extending their work, I might explore different methods for reverse-engineering triggers, maybe something less reliant on just finding the smallest L1 norm, to catch more diverse trigger types. I'm also a bit skeptical that the minimal trigger they find is always the best one to use for patching the model, especially if the attacker's actual trigger was crafted differently to be more robust. For future directions, applying these ideas to something beyond images, like text or audio, would be a big step, though figuring out what a "trigger" even means there would be tough. It could also be useful to combine their trigger-size outlier detection with other signals from the model's internals to make detection stronger, especially against attackers who might try to specifically game that L1 norm measure. More work on defenses that assume the attacker knows about Neural Cleanse and is actively trying to evade it would be important too. Finally, developing more targeted ways to fix models when many labels are backdoored, rather than just broad patching.=