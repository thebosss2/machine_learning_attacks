Paper review 4:

Xu et al. "Instructional fingerprinting of large language models." NAACL. 2024.

Yeah, I thought the paper was pretty solid. It tackles a really relevant problem for LLMs, IP protection, and the instructional fingerprinting idea seems like a clever and practical approach, especially with how they made it persist through fine-tuning. Without any context to any allegedly stolen models(starts with deepsee and ends with k).

So, the core challenge this paper tackles is how to reliably attribute ownership to a foundational LLM after it has been subjected to further fine-tuning by downstream users. Given the significant resources invested in training these models, the paper investigates methods to embed a persistent and verifiable identifier, a "fingerprint", within the LLM. This allows the original publisher to ascertain if another model is a derivative of their work, even after substantial modifications might have obscured its origins.

I think the paper has some clear strengths. First off, it's tackling a really timely and important issue how to protect IP for these big generative LLMs(now there is deepseek 2 or something), which not many previous methods did effectively. They also did a pretty thorough job with their experiments, testing their "Instructional Fingerprinting" on 11 different LLMs and showing it works even after significant fine-tuning on various datasets. Another strong point is that their proposed method, especially the IF_adapter variant, seems quite robust and practical. It's designed to be harmless to model performance, efficient to implement, and hard to accidentally trigger or remove because its in the embbeding layer which users dont tend to finetune on.

One main thing is that for their IF_adapter method to be truly reliable against publisher overclaiming, they suggest needing a trusted third party to manage the fingerprint keys and adapter weights(for anyone to trust that its theirs), which could be complicated to implement legally and practically. They also mention that some of their choices, like the 5:1 ratio for regularization to poison samples, might not be optimal for all models or situations and could need more tuning. Finally, while they show instruction-based fingerprints are resilient, the paper doesn't dive too deeply into the underlying reasons why these specific types of instances are so hard for the model to forget.

Okay, thinking about this further, I'd definitely want to find ways around needing a trusted third party for the IF_adapter(maybe making the key have the name of the model inside, or some other convention). That seems like a big practical hurdle. It'd also be valuable to dig deeper into why instruction-formulated fingerprints are so resistant to removal. understanding that mechanism could lead to even better techniques. Maybe making the SFT black-box method more robust and less prone to guessing would be a significant improvement(so they can't guess and remove it). Additionally, more systematically determining the optimal number and design of fingerprint pairs for various models could make the solution more efficient. And maybe trying to play adverserial and try to remove the fingerprint and see what works.